{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"CDA_NSP.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"f05ZH97QkoJf"},"source":["#Sample training script with naive competitive self-play."]},{"cell_type":"markdown","metadata":{"id":"TcLSdJuUkTrX"},"source":["# Switch directory in Google drive so as to import CDA env.\n"]},{"cell_type":"code","source":["from google.colab import drive \n","drive.mount('/content/gdrive')"],"metadata":{"id":"8WK9AObqL4lD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%cd \"/content/gdrive/My Drive/Github/gym-continuousDoubleAuction/\"\n","!ls -l"],"metadata":{"id":"6kBpZlmeME1e"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# how to use git with colab tutorial: https://medium.com/analytics-vidhya/how-to-use-google-colab-with-github-via-google-drive-68efb23a42d\\\n","# !git config --global user.email \"seangtkelley@gmail.com\"\n","# !git config --global user.name \"Sean Kelley\"\n","# !git remote set-url origin https://PAT@github.com/seangtkelley/gym-continuousDoubleAuction"],"metadata":{"id":"ybS_T9zF078m","executionInfo":{"status":"ok","timestamp":1657050745282,"user_tz":240,"elapsed":328,"user":{"displayName":"Sean Kelley","userId":"04897583237937754138"}}},"execution_count":21,"outputs":[]},{"cell_type":"code","source":["!git status"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SgY9np1502Qj","executionInfo":{"status":"ok","timestamp":1657050634874,"user_tz":240,"elapsed":963,"user":{"displayName":"Sean Kelley","userId":"04897583237937754138"}},"outputId":"7fe48405-2e74-4a2e-d0c5-7de11e1769fc"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["On branch get-notebook-running\n","Changes to be committed:\n","  (use \"git reset HEAD <file>...\" to unstage)\n","\n","\t\u001b[32mmodified:   gym_continuousDoubleAuction/CDA_NSP.ipynb\u001b[m\n","\t\u001b[32mmodified:   gym_continuousDoubleAuction/envs/continuousDoubleAuction_env.py\u001b[m\n","\n","Untracked files:\n","  (use \"git add <file>...\" to include in what will be committed)\n","\n","\t\u001b[31msrc/\u001b[m\n","\n"]}]},{"cell_type":"code","source":["!git checkout -b get-notebook-running"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aq6-JuyH5qlt","executionInfo":{"status":"ok","timestamp":1657050463823,"user_tz":240,"elapsed":6467,"user":{"displayName":"Sean Kelley","userId":"04897583237937754138"}},"outputId":"3cafcc9c-c481-48e9-b5dc-fd5cd3df45ff"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["M\tgym_continuousDoubleAuction/CDA_NSP.ipynb\n","M\tgym_continuousDoubleAuction/envs/continuousDoubleAuction_env.py\n","Switched to a new branch 'get-notebook-running'\n"]}]},{"cell_type":"code","source":["!git add gym_continuousDoubleAuction/"],"metadata":{"id":"CSZQ4j4i6BJL","executionInfo":{"status":"ok","timestamp":1657050630648,"user_tz":240,"elapsed":1440,"user":{"displayName":"Sean Kelley","userId":"04897583237937754138"}}},"execution_count":18,"outputs":[]},{"cell_type":"code","source":["!git commit -m \"fixed class inheritance; added pip deps to beginning of notebook\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"28NDSYGd6Oj_","executionInfo":{"status":"ok","timestamp":1657050640495,"user_tz":240,"elapsed":1002,"user":{"displayName":"Sean Kelley","userId":"04897583237937754138"}},"outputId":"1a5b5332-f427-47cd-ba9f-a7b789c8e3b0"},"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","*** Please tell me who you are.\n","\n","Run\n","\n","  git config --global user.email \"you@example.com\"\n","  git config --global user.name \"Your Name\"\n","\n","to set your account's default identity.\n","Omit --global to set the identity only in this repository.\n","\n","fatal: unable to auto-detect email address (got 'root@d7a113a30ca7.(none)')\n"]}]},{"cell_type":"code","source":["!pip show ray"],"metadata":{"id":"z8p9V0MjNlsr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install ray==0.8.5"],"metadata":{"id":"wIJB38qEP395"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install ray[rllib]==0.8.5"],"metadata":{"id":"qKc8jNIJPTEB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install ray[debug]==0.8.5"],"metadata":{"id":"OWj34JzUPs0P"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install opencv-python-headless==4.1.2.30"],"metadata":{"id":"6LnDnWCjPXyX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"m7ZHcwBWkXVM"},"source":["#Imports"]},{"cell_type":"code","metadata":{"id":"7UW3INjDipTC"},"source":["from __future__ import absolute_import\n","from __future__ import division\n","from __future__ import print_function\n","\n","import os\n","os.environ['RAY_DEBUG_DISABLE_MEMORY_MONITOR'] = \"True\"\n","\n","import argparse\n","import gym\n","import random\n","import numpy as np\n","\n","from collections import defaultdict\n","from typing import Dict\n","\n","import ray\n","from ray import tune\n","from ray.rllib.utils import try_import_tf\n","from ray.tune.registry import register_env\n","from ray.rllib.models import ModelCatalog\n","from ray.rllib.policy import Policy\n","from ray.rllib.agents.ppo import ppo\n","from ray.rllib.agents.ppo.ppo import PPOTrainer\n","from ray.rllib.env import BaseEnv\n","from ray.rllib.env.multi_agent_env import MultiAgentEnv\n","from ray.rllib.policy.sample_batch import SampleBatch\n","from ray.rllib.evaluation import MultiAgentEpisode, RolloutWorker\n","from ray.rllib.agents.callbacks import DefaultCallbacks\n","from ray.tune.logger import pretty_print\n","\n","import sys\n","if \"../\" not in sys.path:\n","    sys.path.append(\"../\")\n","\n","from gym_continuousDoubleAuction.envs.continuousDoubleAuction_env import continuousDoubleAuctionEnv\n","from gym_continuousDoubleAuction.train.model.model_handler import CustomModel_1\n","from gym_continuousDoubleAuction.train.policy.policy_handler import make_RandomPolicy, gen_policy, set_agents_policies, create_train_policy_list\n","from gym_continuousDoubleAuction.train.weight.weight_handler import get_trained_policies_name, get_max_reward_ind, cp_weight\n","from gym_continuousDoubleAuction.train.storage.store_handler import storage\n","from gym_continuousDoubleAuction.train.callbk.callbk_handler import store_eps_hist_data\n","from gym_continuousDoubleAuction.train.logger.log_handler import create_dir, log_g_store, load_g_store\n","from gym_continuousDoubleAuction.train.plotter.plot_handler import plot_storage, plot_LOB_subplot, plot_sum_ord_imb, plot_mid_prices\n","from gym_continuousDoubleAuction.train.helper.helper import ord_imb, sum_ord_imb, mid_price\n","\n","tf = try_import_tf()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SDnpi8k5kbYo"},"source":["#Global\n"]},{"cell_type":"code","metadata":{"id":"UqzjVWUsPykm"},"source":["# CDA_env args\n","num_agents = 9\n","num_trained_agent = 3 # \n","num_policies = num_agents # Each agent is using a separate policy\n","num_of_traders = num_agents\n","tape_display_length = 10 \n","tick_size = 1\n","init_cash = 1000000\n","max_step = 4096 # per episode, -1 in arg. (~7.2s/1000steps/iter)\n","is_render = False \n","\n","# RLlib config \n","train_policy_list = create_train_policy_list(num_trained_agent, \"policy_\")\n","#num_cpus = 0.25                                \n","num_gpus = 0.75 #0                       \n","num_cpus_per_worker = 0.25                                \n","num_gpus_per_worker = 0\n","num_workers = 2\n","num_envs_per_worker = 4\n","batch_mode = \"complete_episodes\" \n","rollout_fragment_length = 128\n","train_batch_size = max_step\n","sgd_minibatch_size = 256\n","num_iters = 1\n","\n","base_dir = os.path.join(\"content\", \"gdrive\", \"My Drive\", \"Github\", \"gym-continuousDoubleAuction\")\n","log_base_dir = os.path.join(base_dir, \"results\")\n","log_dir = os.path.join(log_base_dir, \"ray_results\")\n","\n","# Chkpt & restore\n","local_dir = os.path.join(log_base_dir, \"chkpt\")\n","chkpt_freq = 10\n","chkpt = 320\n","restore_path = os.path.join(local_dir, f\"checkpoint_{chkpt}\", f\"checkpoint-{chkpt}\")\n","is_restore = False # True / False\n","\n","# log & load \n","log_g_store_dir = os.path.join(log_base_dir, \"log_g_store\")\n","# create_dir(restore_path)\n","create_dir(log_base_dir)    \n","create_dir(log_g_store_dir)    \n","\n","# get obs & act spaces from dummy CDA env\n","single_CDA_env = continuousDoubleAuctionEnv(num_of_traders, init_cash, tick_size, tape_display_length, max_step, is_render)\n","obs_space = single_CDA_env.observation_space\n","act_space = single_CDA_env.action_space\n","\n","# register CDA env with RLlib \n","register_env(\"continuousDoubleAuction-v0\", lambda _: continuousDoubleAuctionEnv(num_of_traders, \n","                                                                                init_cash, \n","                                                                                tick_size, \n","                                                                                tape_display_length,\n","                                                                                max_step-1, \n","                                                                                is_render))\n","\n","# register custom model (neural network)\n","ModelCatalog.register_custom_model(\"model_disc\", CustomModel_1) \n","\n","# start ray\n","ray.init(ignore_reinit_error=True, log_to_driver=True, webui_host='127.0.0.1', num_cpus=2) \n","\n","# Global storage, a ray actor that run on it's own process & it needs to be declared after ray.init().\n","g_store = storage.options(name=\"g_store\", detached=True).remote(num_agents)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Cknk9Cnoke_u"},"source":["#Policies"]},{"cell_type":"code","metadata":{"id":"QN5IfMMvP4VA"},"source":["# Dictionary of policies\n","policies = {\"policy_{}\".format(i): gen_policy(i, obs_space, act_space) for i in range(num_policies)}\n","set_agents_policies(policies, obs_space, act_space, num_agents, num_trained_agent)\n","policy_ids = list(policies.keys())\n","\n","def policy_mapper(agent_id):\n","    \"\"\"\n","    Required in RLlib config.\n","    \"\"\"\n","    for i in range(num_agents):\n","        if agent_id == i:            \n","            return \"policy_{}\".format(i)                "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"H4x0UTsxUhys"},"source":["# Call back."]},{"cell_type":"code","metadata":{"id":"kqW_h3UrRs4z"},"source":["class MyCallbacks(DefaultCallbacks):\n","    def on_episode_start(self, worker: RolloutWorker, base_env: BaseEnv,\n","                         policies: Dict[str, Policy],\n","                         episode: MultiAgentEpisode, **kwargs):\n","        \"\"\"\n","        info[\"episode\"] is a MultiAgentEpisode object.\n","\n","        user_data dicts at 100000 items max, will auto replace old with new item at 1st index.\n","        hist_data dicts at 100 items max, will auto replace old with new item at 1st index.\n","        \"\"\"\n","        #print(\"on_episode_start {}, _agent_to_policy {}\".format(episode.episode_id, episode._agent_to_policy))\n","\n","        prefix = \"agt_\"\n","        for i in range(num_agents):\n","            episode.user_data[prefix + str(i) + \"_obs\"] = []\n","            episode.user_data[prefix + str(i) + \"_act\"] = []\n","            episode.user_data[prefix + str(i) + \"_reward\"] = []\n","            episode.user_data[prefix + str(i) + \"_NAV\"] = []\n","            episode.user_data[prefix + str(i) + \"_num_trades\"] = []\n","\n","            episode.hist_data[prefix + str(i) + \"_reward\"] = []\n","            episode.hist_data[prefix + str(i) + \"_NAV\"] = []\n","            episode.hist_data[prefix + str(i) + \"_num_trades\"] = []\n","\n","    def on_episode_step(self, worker: RolloutWorker, base_env: BaseEnv,\n","                        episode: MultiAgentEpisode, **kwargs):\n","        \"\"\"\n","        pole_angle = abs(episode.last_observation_for()[2])\n","        raw_angle = abs(episode.last_raw_obs_for()[2])\n","        assert pole_angle == raw_angle\n","        episode.user_data[\"pole_angles\"].append(pole_angle)\n","        \"\"\"\n","\n","        prefix = \"agt_\"\n","        for i in range(num_agents):\n","            obs = episode.last_raw_obs_for(i)\n","            #obs = episode.last_observation_for(i)\n","            act = episode.last_action_for(i)\n","            reward = episode.last_info_for(i).get(\"reward\")\n","            NAV = episode.last_info_for(i).get(\"NAV\")\n","            NAV = None if NAV is None else float(NAV)\n","            num_trades = episode.last_info_for(i).get(\"num_trades\")\n","        \n","            if reward is None:      # goto next agent.\n","                continue\n","\n","            episode.user_data[prefix + str(i) + \"_obs\"].append(obs)    \n","            episode.user_data[prefix + str(i) + \"_act\"].append(act)    \n","            episode.user_data[prefix + str(i) + \"_reward\"].append(reward)    \n","            episode.user_data[prefix + str(i) + \"_NAV\"].append(NAV)    \n","            episode.user_data[prefix + str(i) + \"_num_trades\"].append(num_trades)          \n","\n","    def on_episode_end(self, worker: RolloutWorker, base_env: BaseEnv,\n","                       policies: Dict[str, Policy], episode: MultiAgentEpisode,\n","                       **kwargs):\n","        #print(\"on_episode_end {}, episode.agent_rewards {}\".format(episode.episode_id, episode.agent_rewards))\n","        \"\"\"\n","        arg: {\"env\": .., \"episode\": ...}\n","        \"\"\"\n","\n","        g_store = ray.util.get_actor(\"g_store\")\n","        prefix = \"agt_\"\n","        for agt_id in range(num_agents):\n","            obs_key = prefix + str(agt_id) + \"_obs\"\n","            act_key = prefix + str(agt_id) + \"_act\"\n","            reward_key = prefix + str(agt_id) + \"_reward\"\n","            NAV_key = prefix + str(agt_id) + \"_NAV\"\n","            num_trades_key = prefix + str(agt_id) + \"_num_trades\"      \n","\n","            # store into episode.hist_data\n","            store_eps_hist_data(episode, reward_key)\n","            store_eps_hist_data(episode, NAV_key)\n","            store_eps_hist_data(episode, num_trades_key)\n","\n","            # store step data\n","            obs = episode.user_data[obs_key]\n","            act = episode.user_data[act_key]\n","            reward = episode.user_data[reward_key]\n","            NAV = episode.user_data[NAV_key]\n","            num_trades = episode.user_data[num_trades_key]\n","            ray.get(g_store.store_agt_step.remote(agt_id, obs, act, reward, NAV, num_trades))       \n","        \n","            # Store episode data.   \n","            eps_reward = np.sum(reward)\n","            eps_NAV = np.sum(NAV)\n","            eps_num_trades = np.sum(num_trades)\n","            ray.get(g_store.store_agt_eps.remote(agt_id, eps_reward, eps_NAV, eps_num_trades))\n","\n","        ray.get(g_store.inc_eps_counter.remote())  \n","\n","    def on_sample_end(self, worker: RolloutWorker, samples: SampleBatch,\n","                      **kwargs):\n","        \"\"\"\n","        arg: {\"samples\": .., \"worker\": ...}\n","\n","        Notes:\n","            https://github.com/ray-project/ray/blob/master/rllib/policy/sample_batch.py        \n","        \"\"\"\n","        #print(\"on_sample_end returned sample batch of size {}\".format(samples.count))\n","\n","        \"\"\"\n","        MultiAgentBatch_obj = info[\"samples\"]\n","        MultiAgentBatch_policy_batches = MultiAgentBatch_obj.policy_batches\n","        MultiAgentBatch_count = MultiAgentBatch_obj.count\n","\n","        access_sample_batches(MultiAgentBatch_policy_batches)\n","        print(\"info['samples'].policy_batches = {}\".format(info[\"samples\"].policy_batches))\n","        print(\"info['worker'] = {}\".format(info[\"worker\"])) # RolloutWorker object\n","        \"\"\"\n","\n","    def on_train_result(self, trainer, result: dict, **kwargs):\n","        \"\"\"\n","        info[\"trainer\"] is the trainer object.\n","\n","        info[\"result\"] contains a bunch of info such as episodic rewards for \n","        each policy in info[\"result\"][hist_stats] dictionary.\n","        \"\"\"\n","        #print(\"trainer.train() result: {} -> {} episodes\".format(trainer, result[\"episodes_this_iter\"]))\n","        # you can mutate the result dict to add new fields to return\n","        result[\"callback_ok\"] = True\n","        #print(\"on_train_result result\", result)\n","    \n","        train_policies_name = get_trained_policies_name(policies, num_trained_agent)    \n","        max_reward_ind = get_max_reward_ind(result, train_policies_name)\n","        max_reward_policy_name = train_policies_name[max_reward_ind]\n","        cp_weight(trainer, train_policies_name, max_reward_policy_name)    \n","\n","        g_store = ray.util.get_actor(\"g_store\")      \n","        prefix = \"policy_policy_\"\n","        suffix = \"_reward\"\n","        hist_stats = result[\"hist_stats\"]\n","        eps_this_iter = result[\"episodes_this_iter\"]\n","        for agt_id in range(num_agents):\n","            key = prefix + str(agt_id) + suffix\n","            for i in range(eps_this_iter):\n","                ray.get(g_store.store_agt_train.remote(agt_id, hist_stats[key][i]))\n","\n","        #print(\"on_train_result info['result'] {}\".format(info[\"result\"]))     \n","\n","    def on_postprocess_trajectory(\n","            self, worker: RolloutWorker, episode: MultiAgentEpisode,\n","            agent_id: str, policy_id: str, policies: Dict[str, Policy],\n","            postprocessed_batch: SampleBatch,\n","            original_batches: Dict[str, SampleBatch], **kwargs):\n","        #print(\"postprocessed {}, {}, {}, {} steps\".format(episode, agent_id, policy_id, postprocessed_batch.count))\n","        \"\"\"\n","        if \"num_batches\" not in episode.custom_metrics:\n","            episode.custom_metrics[\"num_batches\"] = 0\n","        episode.custom_metrics[\"num_batches\"] += 1        \n","\n","        arg: {\"agent_id\": ..., \"episode\": ...,\n","              \"pre_batch\": (before processing),\n","              \"post_batch\": (after processing),\n","              \"all_pre_batches\": (other agent ids)}\n","\n","        # https://github.com/ray-project/ray/blob/ee8c9ff7320ec6a2d7d097cd5532005c6aeb216e/rllib/policy/sample_batch.py\n","        Dictionaries in a sample_obj, k:\n","            t\n","            eps_id\n","            agent_index\n","            obs\n","            actions\n","            rewards\n","            prev_actions\n","            prev_rewards\n","            dones\n","            infos\n","            new_obs\n","            action_prob\n","            action_logp\n","            vf_preds\n","            behaviour_logits\n","            unroll_id       \n","        \"\"\"\n","\n","        \"\"\"\n","        policy_obj = info[\"pre_batch\"][0]\n","        sample_obj = info[\"pre_batch\"][1]    \n","        agt_id = info[\"agent_id\"]\n","        eps_id = info[\"episode\"].episode_id\n","        \"\"\"        "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sEnp5UpxkDve"},"source":["#RLlib config"]},{"cell_type":"code","metadata":{"id":"9fr_a0y6hjn6"},"source":["def get_config():\n","    config = ppo.DEFAULT_CONFIG.copy()\n","    config[\"multiagent\"] = {\"policies_to_train\": train_policy_list,\n","                            \"policies\": policies,\n","                            \"policy_mapping_fn\": policy_mapper,\n","                           }    \n","    #config[\"num_cpus\"] = num_cpus     # trainer, applicable only when using tune.\n","    config[\"num_gpus\"] = num_gpus     # trainer\n","    config[\"num_cpus_per_worker\"] = num_cpus_per_worker                                \n","    config[\"num_gpus_per_worker\"] = num_gpus_per_worker                      \n","    config[\"num_workers\"] = num_workers\n","    config[\"num_envs_per_worker\"] = num_envs_per_worker  \n","    config[\"batch_mode\"] = batch_mode       # \"complete_episodes\" / \"truncate_episodes\"\n","    config[\"train_batch_size\"] = train_batch_size # Training batch size, if applicable. Should be >= rollout_fragment_length.\n","                                                  # Samples batches will be concatenated together to a batch of this size,\n","                                                  # which is then passed to SGD.\n","    config[\"rollout_fragment_length\"] = rollout_fragment_length # replaced \"sample_batch_size\",\n","    config[\"sgd_minibatch_size\"] = sgd_minibatch_size \n","    config[\"log_level\"] = \"WARN\" # WARN/INFO/DEBUG \n","    config[\"callbacks\"] = MyCallbacks\n","    config[\"output\"] = log_dir\n","\n","    return config"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bKLNyViDkI9O"},"source":["#Train\n"]},{"cell_type":"code","metadata":{"id":"iyMEBA0uC8ZW"},"source":["def go_train(config):    \n","    trainer = ppo.PPOTrainer(config=config, env=\"continuousDoubleAuction-v0\")\n","    \n","    if is_restore == True:\n","        trainer.restore(restore_path) \n","\n","    g_store = ray.util.get_actor(\"g_store\")          \n","    result = None\n","    for i in range(num_iters):\n","        result = trainer.train()       \n","        print(pretty_print(result)) # includes result[\"custom_metrics\"]\n","        print(\"training loop = {} of {}\".format(i + 1, num_iters))            \n","        print(\"eps sampled so far {}\".format(ray.get(g_store.get_eps_counter.remote())))\n","\n","        if i % chkpt_freq == 0:\n","            checkpoint = trainer.save(local_dir)\n","            print(\"checkpoint saved at\", checkpoint)\n","    \n","    checkpoint = trainer.save(local_dir)\n","    print(\"checkpoint saved at\", checkpoint)\n","\n","    print(\"result['experiment_id']\", result[\"experiment_id\"])\n","    \n","    return result[\"experiment_id\"]\n","    \n","# run everything\n","experiment_id = go_train(get_config())            "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RmRFarjpD7Wt"},"source":["#Plot all steps.\n","\n","Agt_0, 1, 2 are trained agents (with PPO) while the rest are random agents."]},{"cell_type":"code","metadata":{"id":"wzMc3PY_D7qv"},"source":["plot_storage(num_agents, init_cash)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"e0NDLLyiESgx"},"source":["plot_storage(num_agents, init_cash, \"step\", \"NAV\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GrqwrFwsETlK"},"source":["plot_storage(num_agents, init_cash, \"step\", \"num_trades\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"b-1mV5YFYwkN"},"source":["#Log/load last episode"]},{"cell_type":"code","metadata":{"id":"khD_kiwaJVSQ"},"source":["log_g_store(log_g_store_dir, num_agents, experiment_id)\n","load_g_store(log_g_store_dir, num_agents, experiment_id)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GSu7XmUSDVOE"},"source":["# Plot steps from last episode."]},{"cell_type":"code","metadata":{"id":"rNraawhLmH68"},"source":["plot_storage(num_agents, init_cash)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Lke086wkL4If"},"source":["plot_storage(num_agents, init_cash, \"step\", \"NAV\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2uqneDvkL4x6"},"source":["plot_storage(num_agents, init_cash, \"step\", \"num_trades\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dL1TZkz-Wvqx"},"source":["#LOB from last episode."]},{"cell_type":"code","metadata":{"id":"nR5iU78gCePl"},"source":["g_store = ray.util.get_actor(\"g_store\")          \n","#store = ray.get(g_rere.get_storage.remote())\n","\n","depth = 10\n","bid_size, bid_price, ask_size, ask_price = ray.get(g_store.get_obs_from_agt.remote(0, depth))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rPa_j4M2Z6mh"},"source":["#LOB order imbalance"]},{"cell_type":"code","metadata":{"id":"2PBgQwhTBOqQ"},"source":["ord_imb_store = ord_imb(bid_size, ask_size)\n","plot_LOB_subplot(ord_imb_store, depth, '_ord_imb') "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"v9Kb9NpmaHg4"},"source":["#LOB sum of order imbalance"]},{"cell_type":"code","metadata":{"id":"MXydsI2nC76g"},"source":["ord_imb_store = np.asarray(ord_imb_store)\n","sum_ord_imb_store = sum_ord_imb(ord_imb_store)\n","plot_sum_ord_imb(sum_ord_imb_store, \"sum_ord_imb\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"12iZkIMdaS_l"},"source":["#LOB mid price (subplot)"]},{"cell_type":"code","metadata":{"id":"YhRaxfpg_w2t"},"source":["mid_price_store = mid_price(bid_price, ask_price)\n","plot_LOB_subplot(mid_price_store, depth, '_mid_price')    "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kWPlbUqebcJi"},"source":["#LOB mid prices "]},{"cell_type":"code","metadata":{"id":"CZ724VMdbbWs"},"source":["plot_mid_prices(mid_price_store,)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3JV5v-yYZdqO"},"source":["# LOB bid size"]},{"cell_type":"code","metadata":{"id":"PS-hHA9Mx1CZ"},"source":["plot_LOB_subplot(bid_size, depth, '_bid_size')    "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mUotX7pQZiXP"},"source":["#LOB ask size"]},{"cell_type":"code","metadata":{"id":"iYuL9s5ex5So"},"source":["plot_LOB_subplot(ask_size, depth, '_ask_size')    "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oKiDC3C2ZnW4"},"source":["#LOB bid price"]},{"cell_type":"code","metadata":{"id":"X2qQ-O01x5ER"},"source":["plot_LOB_subplot(bid_price, depth, '_bid_price')    "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"u3HcZH-KZuLd"},"source":["#LOB ask price"]},{"cell_type":"code","metadata":{"id":"FifBnZAdx6uT"},"source":["plot_LOB_subplot(ask_price, depth, '_ask_price')    "],"execution_count":null,"outputs":[]}]}